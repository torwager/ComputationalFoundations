# Recurrent Neural networks

Lecturer: Professor John Murray, Ph.D.

Intro text: Tor Wager, and others (TBD)

## Applications of neural networks in neuroscience

Neural networks have served two main roles in neuroscience. The first, and earliest role, is as a provider of computational principles for how the brain works. Many of the earliest, conceptually powerful demonstrations focused on the kinds of computations that neural networks with even simple architectures *could* perform -- for example:
- Context-dependent response selection (or occasion setting)
- Modeling "double dissociations" without intrinsic functional specialization ([Farah & McClelland 1992](papers/Farah_McClelland.pdf))
- Learning representations of language with a general network architecture rather than specialized "language acquisition modules" (Plaut)
- Rapid generalization to new tasks

In general, there is a premium placed on simplicity here: What is the *simplest* network structure that can solve a given computational task? This provides clues about the necessary computational elements.

The second is as a family of supervised learning algorithms, making predictions about other variables and reproducing outputs generated by humans, including:
- Classification of images or sounds. e.g., image recognition (AlexNet), or emotion recognition (EmoNet)
- Prediction of actions given context/input (e.g., self-driving cars)
- Predictions of the next word or sentence in language (text or code), as in Large Language Models (LLMs) like GPT4, Bard, LLama2, etc.

This second category, by itself, may not tell us very much about the human brain and mind. However, recently, an interesting development is networks that both reproduce an output generated by humans and provide a match between the internal representations of the model and measurable patterns of brain activity (e.g., population-level single neuron recordings or fMRI). While computational tasks (like context-dependent response selection) and human output (like object naming) can be produced in artificial systems in many ways, these "joint objective" models can both produce the output and provide some insight into how the human brain might accomplish the task. Some examples include:

- Using Convolutional Neural Networks (CNNS) to map representations in different ANN model layers to different regions in visual cortex (DiCarlo, Kamitani)
- Using Recurrent Neural Networks (RNNs) to model context-dependent response selection (or other tasks) and compare the representations of nodes to population-level neural recordings ([Mante, Sussillo, et al. 2013 Nature](https://www.nature.com/articles/nature12742); [Churchland, Sussillo, et al. 2015 Nature neuroscience](https://www.nature.com/articles/nn.4042))

## Neural networks: basic terminology

Neurons are abstracted to "nodes" that represent single rate-coded variables. This amounts to abstracting a series of action potentials (on/off responses, 1 or 0) to a continuous rate code. Nodes are grouped into layers, which are sometimes thought of in terms of discrete brain regions or cortical areas (e.g., O'Reilly and Munakata), and layers are connected to one another in hierarchies. Nodes in one layer typically project to those in higher layers, called *feed-forward* projections. They can also project backwards to lower layers via *feedback* projections. Networks with both kinds of projections, or which do not have a clear laminar structure, are *recurrent* neural networks.

There are many types of architecture, or form, that neural networks can take. Nodes in one layer can be *fully connected* to another layer, so that every node in a layer is influenced by (receives input from) every node in a previous layer. They can be *convolutional*, so that a single node receives input from a larger, discrete subset of a previous layer. They can be *sparse*, connecting to only a few units in a receiving layer.  

### Principles of brain function

Historically, neural networks provided the fundamental principles by which the brain represents information structures (e.g., Rumelhart and McClelland).

Some of the key ideas that came from artificial neural networks (ANNs) that are now part of mainstream thinking about how the brain "Works" include:
- Parallel processing: Complex inputs (like images) are broken down into parts (features) and processed in Parallel
- Hierarchical representation: Inputs are processed in a series of layers that sequentially transform them. Early layers represent simple, local features, which are integrated into more complex, abstract, and context-invariant representations in higher layers.
- Distributed representation (Pattern coding): Stimuli (e.g., objects) and other processes (memories, feelings, actions) are represented as distributed codes across patterns of neurons.
  This confers benefits like (a) fill-in from partial information, (b) graded representation, (c) generativity from recombination of patterns
- Backpropagation of error-driven learning: Prediction errors drive updating of weights (connections) to minimize error. These errors are propagated backwards from later layers to earlier layers.

ANNs provide an understanding of how populations of interconnected neurons can produce these wonderful computational properties, and why individual neurons can encode such strange combinations of different stimulus, context, and response features.

## Components of a neural networks

There are many choices to make in specifying and training a neural network, resulting in dizzying flexibility. Some of the basic choices are:

1. Architecture
  a. Number and size of layers
  b. Connectivity pattern: Feedforward and feedback connections, skip connections, convolutional layers  
2. Activation function(s)
  a. Linear
  b. Tanh: A nonlinear activation function
  c. ReLU: Rectified Linear Units: simple linear code with a threshold
3. Loss function
4. Training strategy
5. Error propagation/learning rule or strategy

## Hands-on RNNs with PsychRNN

We'll use Murray Lab's RNN Python package:
[https://github.com/murraylab/PsychRNN](https://github.com/murraylab/PsychRNN)

The easiest way to install this is:
pip install psychrnn

Ehrlich, D. B.*, Stone, J. T.*, Brandfonbrener, D., Atanasov, A., & Murray, J. D. (2021). PsychRNN: An Accessible and Flexible Python Package for Training Recurrent Neural Network Models on Cognitive Tasks. ENeuro, 8(1).

Start with Hello World to get a quick sense of what PsychRNN does. Then go through the Simple Example to get a feel for how to customize PsychRNN. The rest of Getting Started will help guide you through using available features, defining your own task, and even defining your own model.

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1NyT9waI40oFd5hlA_nrWkkcZBwZPfUaJ?usp=sharing)
