

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>General Linear Model (GLM) &#8212; Computational Foundations for Neuroscience</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'glm1';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Parametric inference in the GLM" href="glm2_param_inference.html" />
    <link rel="prev" title="Stats in Matlab" href="statsinmatlab.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
    
    
      
    
    
    <img src="_static/yellow_flower_logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="_static/yellow_flower_logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Computational foundations for Neuroscience
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Preliminary background</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="startingresources.html">Some resources</a></li>

<li class="toctree-l1"><a class="reference internal" href="topicmenu.html">A menu of topics</a></li>
<li class="toctree-l1"><a class="reference internal" href="basicdistancecovcorr.html">Fundamentals of distance, covariance, and correlation</a></li>
<li class="toctree-l1"><a class="reference internal" href="statsinmatlab.html">Stats in Matlab</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">General linear model (GLM)</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">General Linear Model (GLM)</a></li>
<li class="toctree-l1"><a class="reference internal" href="glm2_param_inference.html">Parametric inference in the GLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="glm3_ashardataset.html">Sample dataset and analysis 1: Ashar 2022</a></li>
<li class="toctree-l1"><a class="reference internal" href="glm4_parameterizingglm.html">Parameterizing models in a GLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="glm5_weightedleastsquares.html">Generalized Least Squares (GLS)</a></li>
<li class="toctree-l1"><a class="reference internal" href="glm6_matlab_glm_answerkey.html">A hand-coded GLM in MATLAB</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Mixed effects</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="mixedfx1_intro.html">Mixed effects: Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="mixedfx2_examples.html">Mixed effects: Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="mixedfx3_algebra.html">Mixed effects algebra</a></li>
<li class="toctree-l1"><a class="reference internal" href="mixedfx4_fpr_power_sims.html">Mixed effects simulations</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Multivariate statistics</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="svd.html">Singular Value Decomposition</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Bayesian models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="activeinference_smith.html">Active inference</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Reinforcement Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="reinforcementlearning.html">Reinforcement Learning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Neural networks</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="recurrentneuralnets.html">Recurrent Neural networks</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/torwager/ComputationalFoundations" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/torwager/ComputationalFoundations/edit/master/docs/glm1.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/torwager/ComputationalFoundations/issues/new?title=Issue%20on%20page%20%2Fglm1.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/glm1.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>General Linear Model (GLM)</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-structure">Model Structure</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#error-structure">Error Structure</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-estimation">Model Estimation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#normal-equations">Normal Equations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#estimation-of-coefficients">Estimation of Coefficients</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#residuals">Residuals</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#degrees-of-freedom">Degrees of freedom</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#standard-error-of-hat-beta-in-the-general-linear-model">Standard Error of <span class="math notranslate nohighlight">\(\hat{\beta}\)</span> in the General Linear Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#metrics-of-overall-model-fits">Metrics of overall model fits</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#coefficient-of-determination-r-2">Coefficient of Determination (<span class="math notranslate nohighlight">\(R^2\)</span>)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#akaike-information-criterion-aic-and-bayesian-information-criterion-bic">Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#activity">Activity</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="general-linear-model-glm">
<h1>General Linear Model (GLM)<a class="headerlink" href="#general-linear-model-glm" title="Permalink to this heading">#</a></h1>
<p>The General Linear Model (GLM) is a statistical linear model that models a continuous dependent variable (DV, <span class="math notranslate nohighlight">\(y\)</span> here) as a linear combination of a series of predictors (<span class="math notranslate nohighlight">\(X\)</span>) plus an independent, identically distributed set of errors (<span class="math notranslate nohighlight">\(\epsilon\)</span>). It encompasses a range of models like simple linear regression, multiple regression, and analysis of variance. There are several related concepts:</p>
<ol class="arabic simple">
<li><p><strong>General Linear Model (GLM):</strong></p>
<ul class="simple">
<li><p>The GLM is a broad framework that encompasses many linear models, including simple linear regression, multiple regression, ANOVA, and ANCOVA.</p></li>
<li><p>The assumptions include (1) linear relationships between predictors and the dependent variable, and (2) independent, normally distributed errors with constant variance (homoscedasticity), and the observations are independent.</p></li>
<li><p>The GLM uses the ordinary least squares (OLS) method for estimation.</p></li>
</ul>
</li>
<li><p><strong>Generalized Least Squares (GLS):</strong></p>
<ul class="simple">
<li><p>GLS is an extension of the OLS approach used in the GLM employed when assumptions of homoscedasticity or independence of observations are not met.</p></li>
<li><p>This includes (1) non-constant error variance across groups or levels of a predictor, or (2) autocorrelation due to time-varying processes (errors are not independent)</p></li>
<li><p>In a sense, GLS modifies the model to “correct” for these violations so that the modified model satisfies the OLS assumptions.</p></li>
<li><p>GLS is also used in mixed effects models, when multiple observations are collected on groups of variables (e.g., multiple trials are collected from each of a series of subjects)</p></li>
</ul>
</li>
<li><p><strong>Generalized Linear Model (also abbreviated as GLM):</strong></p>
<ul class="simple">
<li><p>The “generalized linear model”, is another, different extension of the GLM.</p></li>
<li><p>It GLM extends the general linear model to allow for response variables that have error distribution models other than a normal distribution. It includes logistic regression, Poisson regression, etc.</p></li>
<li><p>A link function is introduced in this framework to relate the linear predictor to the mean of the response variable.</p></li>
</ul>
</li>
</ol>
<p>This document describes the GLM with uncorrelated errors.</p>
<p><img alt="GLM tests with uncorrelated errors" src="_images/glmfamily_iid.png" /></p>
<p><a class="reference internal" href="GLM-python.html"><span class="doc std std-doc">GLM-python.ipynb</span></a></p>
<section id="model-structure">
<h2>Model Structure<a class="headerlink" href="#model-structure" title="Permalink to this heading">#</a></h2>
<p>The general formulation of the GLM is:</p>
<div class="math notranslate nohighlight">
\[ y = X\beta + \epsilon \]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( y \)</span> is <span class="math notranslate nohighlight">\(n \times 1\)</span> vector of observed responses (the dependent variable).</p></li>
<li><p><span class="math notranslate nohighlight">\( X \)</span> is the <span class="math notranslate nohighlight">\(n \times p\)</span> design matrix (it contains the values of the predictors, including a column of ones for the intercept).</p></li>
<li><p><span class="math notranslate nohighlight">\( \beta \)</span> is a <span class="math notranslate nohighlight">\(p \times 1\)</span> vector of the parameters or coefficients.</p></li>
<li><p><span class="math notranslate nohighlight">\( \epsilon \)</span> is the <span class="math notranslate nohighlight">\(n \times 1\)</span> vector of errors (residuals).</p></li>
</ul>
<p>Expanded, the matrix multiplication looks like this:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n \\
\end{bmatrix}
=
\begin{bmatrix}
1 &amp; x_{11} &amp; x_{12} &amp; \ldots &amp; x_{1p} \\
1 &amp; x_{21} &amp; x_{22} &amp; \ldots &amp; x_{2p} \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
1 &amp; x_{n1} &amp; x_{n2} &amp; \ldots &amp; x_{np} \\
\end{bmatrix}
*
\begin{bmatrix}
\beta_0 \\
\beta_1 \\
\vdots \\
\beta_p \\
\end{bmatrix}
+
\begin{bmatrix}
\epsilon_1 \\
\epsilon_2 \\
\vdots \\
\epsilon_n \\
\end{bmatrix}
\end{split}\]</div>
<p>This is the same as multiplying each column of <span class="math notranslate nohighlight">\(X\)</span> by it’s corresponding <span class="math notranslate nohighlight">\(\beta\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n \\
\end{bmatrix}
=
\beta_0
*
\begin{bmatrix}
1 \\
1 \\
\vdots \\
1 \\
\end{bmatrix}
+
\beta_1
*
\begin{bmatrix}
x_{11} \\
x_{21} \\
\vdots \\
x_{n1} \\
\end{bmatrix}
+
\beta_2
*
\begin{bmatrix}
x_{12} \\
x_{22} \\
\vdots \\
x_{n2} \\
\end{bmatrix}
\ldots
+
\beta_p
*
\begin{bmatrix}
x_{1p} \\
x_{2p} \\
\vdots \\
x_{np} \\
\end{bmatrix}
+
\begin{bmatrix}
\epsilon_1 \\
\epsilon_2 \\
\vdots \\
\epsilon_n \\
\end{bmatrix}
\end{split}\]</div>
</section>
<section id="error-structure">
<h2>Error Structure<a class="headerlink" href="#error-structure" title="Permalink to this heading">#</a></h2>
<p>The errors <span class="math notranslate nohighlight">\( \epsilon \)</span> are generally assumed to be normally distributed with a mean of zero and a constant variance, <span class="math notranslate nohighlight">\(\sigma^2\)</span>:</p>
<div class="math notranslate nohighlight">
\[ \epsilon \sim N(0, \sigma^2I) \]</div>
<p>Where <span class="math notranslate nohighlight">\( I \)</span> is the identity matrix.</p>
</section>
<section id="model-estimation">
<h2>Model Estimation<a class="headerlink" href="#model-estimation" title="Permalink to this heading">#</a></h2>
<p>The goal of fitting the model is to estimate <span class="math notranslate nohighlight">\( \hat{\beta} \)</span> so that the sum of squared errors (SSE) is minimized. This is:</p>
<div class="math notranslate nohighlight">
\[ S = (y - X\beta)^T(y - X\beta) \]</div>
</section>
<section id="normal-equations">
<h2>Normal Equations<a class="headerlink" href="#normal-equations" title="Permalink to this heading">#</a></h2>
<p>For estimating the coefficients <span class="math notranslate nohighlight">\( \beta \)</span> in the least squares sense, we derive the normal equations. The goal is to minimize the sum of squared residuals:</p>
<div class="math notranslate nohighlight">
\[ S = (y - X\beta)^T(y - X\beta) \]</div>
<p>Taking the derivative with respect to <span class="math notranslate nohighlight">\( \beta \)</span> and setting it to zero gives the normal equations:</p>
<div class="math notranslate nohighlight">
\[ X^TX\beta = X^Ty \]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(X'X\)</span> is called the Gram matrix, which is a symmetric matrix related to the predictor covariance. It is related to the Fisher Information, which in turn is related to the precision with which each parameter can be estimated.</p></li>
<li><p><span class="math notranslate nohighlight">\(X'Y\)</span> is a product of the transpose of the design matrix and the response vector.</p></li>
</ul>
</section>
<section id="estimation-of-coefficients">
<h2>Estimation of Coefficients<a class="headerlink" href="#estimation-of-coefficients" title="Permalink to this heading">#</a></h2>
<p>By solving the normal equations, given that <span class="math notranslate nohighlight">\( X^TX \)</span> is invertible, we get estimates of the coefficients <span class="math notranslate nohighlight">\( \beta \)</span>:</p>
<div class="math notranslate nohighlight">
\[ \hat{\beta} = (X^TX)^{-1}X^Ty \]</div>
<p>Where <span class="math notranslate nohighlight">\( \hat{\beta} \)</span> represents the estimated coefficients that minimize the sum of squared residuals.</p>
<p>Geometrically, this is the <strong>orthogonal projection</strong> of <span class="math notranslate nohighlight">\(y\)</span> onto the model subspace, a <span class="math notranslate nohighlight">\(p\)</span>-dimensional space defined by the columns of <span class="math notranslate nohighlight">\(X\)</span>.</p>
<p>The <strong>model fits</strong> <span class="math notranslate nohighlight">\(X\hat{\beta}\)</span> provide the fitted values, the best linear combination of columns of <span class="math notranslate nohighlight">\(X\)</span>, where “best” minimizes the SSE.</p>
<p>The figure below illustrates this for a dataset with 3 data points, meaning <span class="math notranslate nohighlight">\(y\)</span> is a 3-length vector describing a point in 3-D space. This extends to n-D space with <span class="math notranslate nohighlight">\(n\)</span> independent observations. The axes are orthogonal if the observations are independent (as assumed in the GLM). <span class="math notranslate nohighlight">\(X\)</span> has 2 columns and spans a 2-D space (a plane). The fitted values are contained in the vector closest to the data that lies in the model subspace (in the <strong>image</strong> in algebraic terms). The error values are contained in the vector orthogonal to the fits (in the <strong>kernel</strong> in algebra) that, when added to the fits, matches the data.</p>
<p><img alt="Geometry of GLM fits" src="_images/glm_geometry.png" />
From “Practical Regression and Anova using R” by Julian Faraway, 2002</p>
<p>Here’s my version, with color:
<img alt="Geometry of GLM fits, tor's version" src="_images/glm_geometry_tor.png" /></p>
<p><strong>multicolinearity</strong>
Importantly, the projection estimates the best overall linear combination. In an algebraic sense, the <span class="math notranslate nohighlight">\(X_{.i}\hat{\beta}\)</span> partial fits combine to reach the point in the model subspace closest to the data <span class="math notranslate nohighlight">\(y\)</span>. Each <span class="math notranslate nohighlight">\(\hat{\beta_i}\)</span> is thus estimated “controlling for” the other predictors, and are often interpreted as the <strong>unique effects</strong> that cannot be accounted for by other predictors in the model.</p>
<ul class="simple">
<li><p>If any of the predictors in <span class="math notranslate nohighlight">\(X\)</span> is a perfect linear combination of the other predictors, <span class="math notranslate nohighlight">\(X\)</span> is <strong>rank-deficient</strong> and does not span the a subspace equal to the number of predictors. In this case, there is no unique solution for <span class="math notranslate nohighlight">\(\hat{\beta}\)</span>, and the matrix <span class="math notranslate nohighlight">\( (X^TX)^{-1} \)</span> is not invertible.</p></li>
<li><p>If predictors are correlated but still linearly independent, the variance increases in proportion to the correlation.</p></li>
</ul>
<p>The following movies show a simulation with two predictors (X1 and X2) and a DV (y, Outcome). In both, there are strong positive effects of both predictors.  They differ only in the correlation between X1 and X2.  In the first, they are uncorrelated, and we can see that the regression plane is stable across repeated samples
:</p>
<p><img alt="Regression with 2 predictors" src="_images/movie_regression_2preds.gif" /></p>
<p>In the second, the two predictors are correlated.  The regression plane tips on a ridge defined by the correlated predictors, and is unstable across repeated samples:</p>
<p><img alt="Regression with 2 predictors" src="_images/movie_regression_colinear.gif" /></p>
<p><a class="reference download internal" download="" href="_downloads/e33d68312cb0b04dd8838a72d3952aa9/movie_regression_2preds.m"><span class="xref download myst">Matlab code for sim 1</span></a>
<a class="reference download internal" download="" href="_downloads/946d0f57ca71b77b5ca3f365ae30d71d/movie_regression_colinear.m"><span class="xref download myst">and sim 2</span></a></p>
</section>
<section id="residuals">
<h2>Residuals<a class="headerlink" href="#residuals" title="Permalink to this heading">#</a></h2>
<p>Residuals are differences between the observed values and the values predicted by the model. If a model fits well, the residuals should be randomly scattered around zero.</p>
<div class="math notranslate nohighlight">
\[ e_i = y_i - \hat{y}_i \]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(e_i\)</span> is the residual for observation <span class="math notranslate nohighlight">\(i\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(y_i\)</span> is the observed value for observation <span class="math notranslate nohighlight">\(i\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{y}_i\)</span> is the predicted value for observation <span class="math notranslate nohighlight">\(i\)</span>.</p></li>
</ul>
<p>The residual variance is:</p>
<div class="math notranslate nohighlight">
\[
var(e) = \frac{1}{n-1} \sum_{i=1}^{n} {(e_i)^2} = \frac{S}{n-1}
\]</div>
<p>The residual standard deviation is <span class="math notranslate nohighlight">\( \sqrt{var(e)} \)</span>. It is a measure of the average deviation of observations from the model’s predictions.</p>
</section>
<section id="degrees-of-freedom">
<h2>Degrees of freedom<a class="headerlink" href="#degrees-of-freedom" title="Permalink to this heading">#</a></h2>
<p>The degrees of freedom (df) is the number of dimensions along which a quantity can vary. In the GLM, df are associated with both the model and the error. The model df is the dimensionality of the space spanned by the combination of parameters <span class="math notranslate nohighlight">\(\beta\)</span>. The error df, or dfe, is the dimensionality of the space spanned by <span class="math notranslate nohighlight">\(\epsilon\)</span>.</p>
<p>In both cases, the dimensionality is equal to the number of independent parameters or errors. In the case of model df, it is generally equal to the number of parameters <span class="math notranslate nohighlight">\(p\)</span>, assuming none of the predictors in X are redundant. Here, <span class="math notranslate nohighlight">\(p\)</span> includes a parameter for the intercept, so <span class="math notranslate nohighlight">\(p=1\)</span> for an intercept-only model.</p>
<p>In the case of error df, assuming the errors are independent, it is the number of observations (<span class="math notranslate nohighlight">\(n\)</span>) minus the number of model parameters estimated, including the intercept (<span class="math notranslate nohighlight">\(p\)</span>). For example, if there are 3 independent observations and one model parameter estimated (the intercept, which estimates the sample mean in this case), there are only 2 independent dimensions along which the errors can vary. (Once you know the mean and two observations, you can calculate the third). In algebraic terms they span a two-dimensional plane. Thus, the dfe are n - 1 in this simple case.  This is the ReML estimator for the sample variance.</p>
</section>
<section id="standard-error-of-hat-beta-in-the-general-linear-model">
<h2>Standard Error of <span class="math notranslate nohighlight">\(\hat{\beta}\)</span> in the General Linear Model<a class="headerlink" href="#standard-error-of-hat-beta-in-the-general-linear-model" title="Permalink to this heading">#</a></h2>
<p>In the context of the General Linear Model, the standard errors of the estimated coefficients, <span class="math notranslate nohighlight">\(\hat{\beta}\)</span>s, quantifies the uncertainty associated with the estimates. They provide crucial information for hypothesis testing and for constructing confidence intervals around <span class="math notranslate nohighlight">\(\hat{\beta}\)</span>s.</p>
<p>The standard error is a measure of the expected average deviation of the model’s parameter estimates <span class="math notranslate nohighlight">\(\hat{\beta}\)</span> given the sampling error <span class="math notranslate nohighlight">\( var(e) \)</span>. In general, it is related to <span class="math notranslate nohighlight">\( var(e) / sqrt(dfe) \)</span>, where <span class="math notranslate nohighlight">\(dfe\)</span> is the error degrees of freedom. This is due to the Central Limit Theorem, which posits that the error of an estimate is inversely proportional to <span class="math notranslate nohighlight">\( \sqrt{n} \)</span>.</p>
<p>The covariance of <span class="math notranslate nohighlight">\(\hat{\beta}\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[ \text{Var}(\hat{\beta}) = \sigma^2 (X^T X)^{-1} \]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\sigma^2\)</span> is the variance of the residuals.</p></li>
<li><p><span class="math notranslate nohighlight">\(X\)</span> is the design matrix.</p></li>
</ul>
<p>This is a variance/covariance matrix.</p>
<p>From this, the standard error (SE) of each <span class="math notranslate nohighlight">\(\hat{\beta}_i, i=1...p\)</span> is the square root of its variance:</p>
<div class="math notranslate nohighlight">
\[ SE(\hat{\beta_i}) = \sqrt{\text{Var}(\hat{\beta_{ii}})} \]</div>
<p>where <span class="math notranslate nohighlight">\(ii\)</span> denotes the <span class="math notranslate nohighlight">\(i\)</span>th diagonal element of the <span class="math notranslate nohighlight">\(p x p\)</span> matrix <span class="math notranslate nohighlight">\( \text{Var}(\hat{\beta}) \)</span>.</p>
<p>The standard error provides a measure of the variability or dispersion of the estimator. A smaller standard error indicates a more stable estimator. When conducting hypothesis tests, a larger standard error (relative to the magnitude of <span class="math notranslate nohighlight">\(\hat{\beta}\)</span>) might make it harder to reject the null hypothesis that the coefficient is zero, since the coefficient estimate is less precise.</p>
<p>More precisely, the <span class="math notranslate nohighlight">\( SE(\hat{\beta_i}) \)</span> is the denominator of the t-statistic that forms the basis for parametric inferences (hypothesis tests).</p>
<p>The standard error is influenced by the sample size. With larger sample sizes, the standard error tends to decrease, which leads to more precise estimates. If observations are not indepdendent and errors are correlated, this will also increase the standard error of estimates, as we have less data than we think we have.</p>
<p>Standard errors and degrees of freedom are used in statistical inference.</p>
</section>
<section id="metrics-of-overall-model-fits">
<h2>Metrics of overall model fits<a class="headerlink" href="#metrics-of-overall-model-fits" title="Permalink to this heading">#</a></h2>
<p>Model fit indicates how well a statistical model describes the observed data. When creating predictive models, we want our model to represent the underlying data structure accurately. There are various measures and statistics to gauge the fit of a model.</p>
<section id="coefficient-of-determination-r-2">
<h3>Coefficient of Determination (<span class="math notranslate nohighlight">\(R^2\)</span>)<a class="headerlink" href="#coefficient-of-determination-r-2" title="Permalink to this heading">#</a></h3>
<p>The coefficient of determination, often denoted as <span class="math notranslate nohighlight">\(R^2\)</span>, measures the proportion of variance in the dependent variable that is predictable from the independent variable(s).</p>
<div class="math notranslate nohighlight">
\[ R^2 = 1 - \frac{SS_{\text{res}}}{SS_{\text{tot}}} \]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(SS_{\text{res}}\)</span> is the residual sum of squares, <span class="math notranslate nohighlight">\(e^Te\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(SS_{\text{tot}}\)</span> is the total sum of squares, adjusting for the mean, <span class="math notranslate nohighlight">\((y-\bar{y})^T(y-\bar{y})\)</span>.</p></li>
</ul>
<p>A value of <span class="math notranslate nohighlight">\(R^2\)</span> close to 1 suggests that a large proportion of the variability in the outcome has been explained by the regression model. A low <span class="math notranslate nohighlight">\(R^2\)</span> indicates otherwise.</p>
</section>
<section id="akaike-information-criterion-aic-and-bayesian-information-criterion-bic">
<h3>Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC)<a class="headerlink" href="#akaike-information-criterion-aic-and-bayesian-information-criterion-bic" title="Permalink to this heading">#</a></h3>
<p>Both AIC and BIC are criteria used for model selection. They consider the goodness of fit of the model and the complexity of the model.</p>
<p>Lower values of AIC or BIC suggest a better-fitting model. When comparing multiple models, the one with the lowest AIC or BIC can often be considered the best fit, taking into account both the goodness of fit and model complexity.</p>
</section>
</section>
<section id="activity">
<h2>Activity<a class="headerlink" href="#activity" title="Permalink to this heading">#</a></h2>
<p>Code the GLM in matrix form from scratch using your platform of choice.
Generally, this will be Python, R, or Matlab.</p>
<p>Include expressions for calculating <span class="math notranslate nohighlight">\(\hat{\beta}\)</span>, <span class="math notranslate nohighlight">\(var(\beta)\)</span>, dfe, and <span class="math notranslate nohighlight">\(R^2\)</span>.</p>
<p>Note: We will validate this later and apply it to a real dataset (Ashar et al. 2022) later.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="statsinmatlab.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Stats in Matlab</p>
      </div>
    </a>
    <a class="right-next"
       href="glm2_param_inference.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Parametric inference in the GLM</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-structure">Model Structure</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#error-structure">Error Structure</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-estimation">Model Estimation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#normal-equations">Normal Equations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#estimation-of-coefficients">Estimation of Coefficients</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#residuals">Residuals</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#degrees-of-freedom">Degrees of freedom</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#standard-error-of-hat-beta-in-the-general-linear-model">Standard Error of <span class="math notranslate nohighlight">\(\hat{\beta}\)</span> in the General Linear Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#metrics-of-overall-model-fits">Metrics of overall model fits</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#coefficient-of-determination-r-2">Coefficient of Determination (<span class="math notranslate nohighlight">\(R^2\)</span>)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#akaike-information-criterion-aic-and-bayesian-information-criterion-bic">Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#activity">Activity</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Tor Wager & others
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>